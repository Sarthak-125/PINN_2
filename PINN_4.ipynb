
Open In Colab
Importing libraries

In [4]:
import numpy as np
import tensorflow as  tf
import random
BC u(t,1) = u(t,-1) = 0

Collocation point creation

In [5]:
n_c = 5000  #collocation points

def Rand(start, end, num): 
    res = [] 
  
    for j in range(num): 
        res.append(random.uniform(start, end)) 
  
    return res

x_c = Rand(-1, 1, n_c)
t_c = Rand(0, 1, n_c)
x_c = tf.cast(x_c ,dtype='float64')
t_c = tf.cast(t_c ,dtype='float64')
input_c = tf.stack((x_c, t_c), axis=1)
x_bc_u = np.ones((n_c,), dtype='float64')
x_bc_b = - x_bc_u
x_bc_u = tf.cast(x_bc_u, dtype='float64')
x_bc_b = tf.cast(x_bc_b, dtype='float64')
input_bcu = tf.stack((x_bc_u, t_c), axis = 1)
input_bcb = tf.stack((x_bc_b, t_c), axis = 1)
input_bc = tf.concat((input_bcu, input_bcb), axis=0)
input_f = tf.concat((input_c, input_bc), axis=0)
x_f = input_f[:,0:1]
t_f = input_f[:,1:2]
y_bc = 0
out_bc = (y_bc* np.ones((2*n_c,1), dtype='float64'))
out_bc = tf.cast(out_bc, dtype='float64')

Architecture ANN

In [7]:
x_in = tf.keras.layers.Input(shape=(1,1), name="Position")
t_in = tf.keras.layers.Input(shape=(1,1), name="Time")
IN = tf.keras.layers.Concatenate(axis=-1, name="Input")([x_in, t_in])
dense1=(tf.keras.layers.Dense(50,activation='tanh'))(IN)
dense2=(tf.keras.layers.Dense(50,activation='tanh'))(dense1)
dense3=(tf.keras.layers.Dense(50,activation='tanh'))(dense2)
dense4=(tf.keras.layers.Dense(50,activation='tanh'))(dense3)
dense5=(tf.keras.layers.Dense(50,activation='tanh'))(dense4)
output=(tf.keras.layers.Dense(1,activation='tanh'))(dense5)
pinn1 = tf.keras.Model(inputs=[x_in,t_in], outputs=output, name="pinn1")
Custom Loss function

In [11]:
def custom_loss_1(arch, x, t):
  def loss(u_train, u_pred):
    with tf.GradientTape(persistent=True) as t1:
      t1.watch(x)
      t1.watch(t)
      with tf.GradientTape() as t2:
        t2.watch(x)
        u = arch([x, t])
        u_x = t2.gradient(u, x)
      u_t = t1.gradient(u, t)
      u_xx = t1.gradient(u_x, x)
    u_pred = tf.cast(u_pred, dtype='float64')
    u_train = tf.cast(u_train, dtype='float64')      
    f = u_t + u*u_x - (0.01/np.pi)*u_xx
    loss_res = tf.reduce_mean(tf.square(f))
    loss_2 = tf.reduce_mean(tf.square(u_pred-u_train))
    return loss_res + loss_2
  return loss


loss_fn = custom_loss_1(pinn1, x_f, t_f)
pinn1.compile(optimizer='adam', loss=loss_fn, metrics=['mse'])
pinn1.fit([input_bc[:,0:1], input_bc[:,1:2]], out_bc, batch_size=32, epochs=1000)
Epoch 1/1000
WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-11-a58f547843b5> in <module>()
     21 loss_fn = custom_loss_1(pinn1, x_f, t_f)
     22 pinn1.compile(optimizer='adam', loss=loss_fn, metrics=['mse'])
---> 23 pinn1.fit([input_bc[:,0:1], input_bc[:,1:2]], out_bc, batch_size=32, epochs=1000)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1098                 _r=1):
   1099               callbacks.on_train_batch_begin(step)
-> 1100               tmp_logs = self.train_function(iterator)
   1101               if data_handler.should_sync:
   1102                 context.async_wait()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    826     tracing_count = self.experimental_get_tracing_count()
    827     with trace.Trace(self._name) as tm:
--> 828       result = self._call(*args, **kwds)
    829       compiler = "xla" if self._experimental_compile else "nonXla"
    830       new_tracing_count = self.experimental_get_tracing_count()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    869       # This is the first call of __call__, so we have to initialize.
    870       initializers = []
--> 871       self._initialize(args, kwds, add_initializers_to=initializers)
    872     finally:
    873       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    724     self._concrete_stateful_fn = (
    725         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 726             *args, **kwds))
    727 
    728     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2967       args, kwargs = None, None
   2968     with self._lock:
-> 2969       graph_function, _ = self._maybe_define_function(args, kwargs)
   2970     return graph_function
   2971 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3359 
   3360           self._function_cache.missed.add(call_context_key)
-> 3361           graph_function = self._create_graph_function(args, kwargs)
   3362           self._function_cache.primary[cache_key] = graph_function
   3363 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3204             arg_names=arg_names,
   3205             override_flat_arg_shapes=override_flat_arg_shapes,
-> 3206             capture_by_value=self._capture_by_value),
   3207         self._function_attributes,
   3208         function_spec=self.function_spec,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    988         _, original_func = tf_decorator.unwrap(python_func)
    989 
--> 990       func_outputs = python_func(*func_args, **func_kwargs)
    991 
    992       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    632             xla_context.Exit()
    633         else:
--> 634           out = weak_wrapped_fn().__wrapped__(*args, **kwds)
    635         return out
    636 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    975           except Exception as e:  # pylint:disable=broad-except
    976             if hasattr(e, "ag_error_metadata"):
--> 977               raise e.ag_error_metadata.to_exception(e)
    978             else:
    979               raise

TypeError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *
        return step_function(self, iterator)
    <ipython-input-11-a58f547843b5>:14 loss  *
        f = u_t + u*u_x - (0.01/np.pi)*u_xx
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper
        raise e
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper
        return func(x, y, name=name)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1496 _mul_dispatch
        return multiply(x, y, name=name)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:518 multiply
        return gen_math_ops.mul(x, y, name)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:6078 mul
        "Mul", x=x, y=y, name=name)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper
        inferred_from[input_arg.type_attr]))

    TypeError: Input 'y' of 'Mul' Op has type float64 that does not match type float32 of argument 'x'.
